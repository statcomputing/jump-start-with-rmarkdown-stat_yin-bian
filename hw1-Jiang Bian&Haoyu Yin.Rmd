---
title: "Approximation of The Distribution Function of N(0,1) by Using The Monte Carlo Methods"
author: "Jiang Bian & Haoyu Yin"
date: "`r format(Sys.time(), '%d %B %Y')`"
documentclass: article
papersize: letter
fontsize: 11pt
output: pdf_document
abstract: |
    This is an approximation of the distribution function of N(0,1) by using the Monte Carlo Methods.
    To do the experiment, we set up a function with specific values of two parameters, and for each pair 
    of those parameters, we repeat the experiment 100 times to compare the results with the true values 
    of the function that we want to approximate.
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
if (!require("knitr")) 
  install.packages("knitr",dependencies = TRUE) 

## specify global chunk options
knitr::opts_chunk$set(fig.width = 16, fig.height = 5, dpi = 300,
                      out.width = "90%", fig.align = "center")
```

## Math Equations

Original Function
$$
    \Phi (t)=\int_{-\infty}^{t} \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy. 
$$

Approximated Function
$$
    \widehat{\Phi} (t)=\frac{1}{n}\sum_{i=1}^{n}I(X_i\leq t).
$$

## Methodology

We do the experiment with the approximation at $n \in \left \{ 10^2,10^3,10^4 \right \}$ 

at $t \in \left \{0.0,0.67,0.84,1.28,1.65,2.32,2.58,3.09,3.72 \right \}$

```{r nt}
n <- c(100, 1000, 10000)
t <- c(0.0,0.67,0.84,1.28,1.65,2.32,2.58,3.09,3.72)
```

Then, we use for loop function to generate 27 different combinations of n and t and each repeat 100 times.

We receive:

``` {r loop, echo=FALSE}
result <- array(rep(0,100*length(n)*length(t)), c(length(n), 100, length(t)))
tvalue <- array(rep(0,100*length(n)*length(t)), c(length(n), 100, length(t)))
diff <- array(rep(0,100*length(n)*length(t)), c(length(n), 100, length(t)))

j <- 1
for (j in 1:3) {
  k <- 1
  for (k in 1:9) {
    r <-  1
    for (r in 1:100){
      phi <- rnorm(n[j], 0, 1)
      test <- rep(0,n[j])
      for (i in 1:n[j]){
        if (phi[i] <= t[k]) {
          test[i] = 1
        } else {
          test[i] = 0
        }
      }
      result[j,r,k] <- sum(test)/n[j]
      tvalue[j,r,k] <- pnorm(t[k], 0, 1)
      diff[j,r,k] <- result[j,r,k] - tvalue[j,r,k]
      r <- r + 1
    }
    k <- k + 1
  }
  j <- j + 1
}

dimnames(diff)[[3]] <- t

```

When n = 100,
```{r table1}
knitr::kable(head(result[1,,]), digits = 4, col.names = t)
```

When n = 1000,
```{r table2}
knitr::kable(head(result[2,,]), digits = 4, col.names = t)
```

When n = 10000,
```{r table3}
knitr::kable(head(result[3,,]), digits = 4, col.names = t)
```

For comparision, the difference between valuations and true values are:

When n = 100,
```{r com1}
knitr::kable(head(diff[1,,]), digits = 4, col.names = t)
```

When n = 1000,
```{r com2}
knitr::kable(head(diff[2,,]), digits = 4, col.names = t)
```

When n = 10000,
```{r com3}
knitr::kable(head(diff[3,,]), digits = 4, col.names = t)
```

## Plots

We draw the box plots to show the bias at all t.

```{r plotting}
m <- 1
for (m in 1:length(n)){
  boxplot(diff[m,,])
  m <- m + 1
}
```

## Conclusion

In conclusion, we found that the approximation of the function that 
we set up is ideal and when n becomes bigger, the result is more accurate.
